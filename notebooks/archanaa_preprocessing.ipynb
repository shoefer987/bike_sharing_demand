{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import requests\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from bikesharing.params import *\n",
    "from bikesharing.ml_logic.data import get_raw_data\n",
    "from bikesharing.ml_logic.encoders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing functions ##\n",
    "\n",
    "def pre_process_rental_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the rental DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Select relevant columns only\n",
    "    df = df[['STARTTIME', 'STARTLAT', 'STARTLON']].copy()\n",
    "\n",
    "    # Strip column names\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "    # Remove column 'Row'\n",
    "    df.drop(columns='Row', inplace=True, errors='ignore')\n",
    "\n",
    "    # Make string replacements\n",
    "    df_obj = df.select_dtypes(include='object')\n",
    "    df[df_obj.columns] = df_obj.applymap(lambda x: x.strip().replace(',', '.') if isinstance(x, str) else x)\n",
    "\n",
    "    # Handle datetime and numerical datatypes\n",
    "    df.STARTTIME = pd.to_datetime(df.STARTTIME)\n",
    "    df[['STARTLAT', 'STARTLON']] = df[['STARTLAT', 'STARTLON']].astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_polygons(polygons_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads polygons from a file and returns them as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        polygons_file (str): The path to the polygons file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The dictionary of polygons.\n",
    "    \"\"\"\n",
    "    polygons = {}\n",
    "    with open(polygons_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            polygons[row['district']] = Polygon(json.loads(row['coordinates']))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def get_district(rental_df: pd.DataFrame, polygons: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a spatial join between the rental DataFrame and polygons.\n",
    "\n",
    "    Args:\n",
    "        rental_df (pd.DataFrame): The rental DataFrame.\n",
    "        polygons (dict): The dictionary of polygons.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the spatial join result.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the polygons dictionary\n",
    "    polygons_df = pd.DataFrame.from_dict(polygons, orient='index', columns=['geometry'])\n",
    "    # Reset the index to make the 'district' column a regular column\n",
    "    polygons_df = polygons_df.reset_index().rename(columns={'index': 'district'})\n",
    "\n",
    "    # Create a GeoDataFrame from the polygons DataFrame\n",
    "    polygons_gdf = gpd.GeoDataFrame(polygons_df)\n",
    "    # Set the geometry column in the polygons_gdf GeoDataFrame\n",
    "    polygons_gdf.set_geometry('geometry', inplace=True)\n",
    "\n",
    "    # Create a GeoDataFrame from the point data\n",
    "    geometry = [Point(row['STARTLON'], row['STARTLAT']) for _, row in rental_df.iterrows()]\n",
    "    rental_gdf = gpd.GeoDataFrame(rental_df, geometry=geometry)\n",
    "    # Set the geometry column in the rental_gdf GeoDataFrame\n",
    "    rental_gdf.set_geometry('geometry', inplace=True)\n",
    "\n",
    "    # Perform the spatial join\n",
    "    rental_geo_df = gpd.sjoin(rental_gdf, polygons_gdf, predicate='within')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    rental_geo_df = rental_geo_df.drop(columns=['geometry', 'index_right'])\n",
    "\n",
    "    return rental_geo_df\n",
    "\n",
    "\n",
    "def encode_district_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encodes the district labels in the DataFrame using one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with encoded district labels.\n",
    "    \"\"\"\n",
    "    # Instantiate the OneHotEncoder\n",
    "    district_ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Fit encoder\n",
    "    district_ohe.fit(df[['district']])\n",
    "\n",
    "    # Apply one-hot encoding and add the encoded columns to the DataFrame\n",
    "    encoded_columns = district_ohe.get_feature_names_out()\n",
    "    encoded_values = district_ohe.transform(df[['district']])\n",
    "    df_encoded = pd.DataFrame(encoded_values, columns=encoded_columns)\n",
    "\n",
    "    # Update the column names in df without the prefix 'district_'\n",
    "    column_names = [column.split('district_', 1)[-1] for column in encoded_columns]\n",
    "    df_encoded.columns = list(df.columns[:-len(encoded_columns)]) + column_names\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def group_rental_data_by_hour(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups the rental data by hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with rental data grouped by hour.\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    df['date'] = df['STARTTIME'].dt.date\n",
    "    df['year'] = df['STARTTIME'].dt.year\n",
    "    df['month'] = df['STARTTIME'].dt.month\n",
    "    df['hour'] = df['STARTTIME'].dt.hour\n",
    "    df['date_hour'] = df['STARTTIME'].dt.floor('H')\n",
    "\n",
    "    # Grouping by Hour\n",
    "    df_by_hour = df.groupby('date_hour').agg({\n",
    "        'RENTAL_IS_STATION': np.mean,\n",
    "        'year': np.mean,\n",
    "        'month': np.mean,\n",
    "        'hour': np.mean,\n",
    "        **{district: np.sum for district in df['district']}\n",
    "    }).reset_index()\n",
    "\n",
    "    return df_by_hour\n",
    "\n",
    "\n",
    "def get_weather_info(rental_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves weather information and merges it with the rental data.\n",
    "\n",
    "    Args:\n",
    "        rental_df (pd.DataFrame): The rental DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with merged rental and weather data.\n",
    "    \"\"\"\n",
    "    def fetch_weather_data(latitude, longitude, start_date, end_date):\n",
    "        # TODO: should be moved to env\n",
    "        url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "        params = {\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'hourly': ['temperature_2m', 'relativehumidity_2m', 'apparent_temperature', 'windspeed_10m', 'precipitation']\n",
    "        }\n",
    "        weather_data = requests.get(url, params=params).json()\n",
    "        df_weather = pd.DataFrame(weather_data['hourly'])\n",
    "        df_weather['time'] = pd.to_datetime(df_weather['time'])\n",
    "\n",
    "        return df_weather\n",
    "\n",
    "    # Merge rental data with weather data\n",
    "    df_weather = fetch_weather_data()\n",
    "    merged_data = rental_df.merge(df_weather, left_on='date_hour', right_on='time', how='left').drop(columns='time')\n",
    "    merged_data['date_hour'] = pd.to_datetime(merged_data['date_hour'])\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def feature_extraction(data: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs feature engineering on the input data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the features DataFrame (X) and the target DataFrame (y).\n",
    "    \"\"\"\n",
    "    data['date_hour'] = pd.to_datetime(data['date_hour'])\n",
    "\n",
    "    # Extract date from date_hour\n",
    "    data['date'] = data['date_hour'].dt.date\n",
    "\n",
    "    # Select features for X and y\n",
    "    X = data[['date', 'date_hour', 'year', 'month', 'hour', 'temperature_2m', 'relativehumidity_2m',\n",
    "              'apparent_temperature', 'windspeed_10m', 'precipitation']]\n",
    "    \n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    X['is_weekend'] = X['date'].dt.weekday >= 5\n",
    "\n",
    "    bay_holidays = holidays.CountryHoliday('DE', prov='BY')\n",
    "    X['is_holiday'] = X['date'].apply(lambda x: x in bay_holidays)\n",
    "    \n",
    "    y = data[['date_hour','Sendling-Westpark', 'Altstadt-Lehel', 'Schwabing-West', 'Untergiesing',\n",
    "       'Untergiesing-Harlaching', 'Maxvorstadt', 'Bogenhausen', 'Sendling',\n",
    "       'Milbertshofen-Am Hart', 'Neuhausen-Nymphenburg', 'Moosach',\n",
    "       'Obergiesing', 'Au - Haidhausen', 'Ludwigsvorstadt-Isarvorstadt',\n",
    "       'Laim', 'Schwanthalerhöhe', 'Schwabing-Freimann', 'Ramersdorf-Perlach',\n",
    "       'Thalkirchen', 'Aubing-Lochhausen-Langwied', 'Hadern', 'Berg am Laim',\n",
    "       'Harlaching', 'Obersendling', 'Südgiesing', 'Pasing',\n",
    "       'Pasing-Obermenzing', 'Hasenbergl-Lerchenau Ost', 'Obermenzing',\n",
    "       'Trudering', 'Trudering-Riem', 'Feldmoching', 'Untermenzing-Allach',\n",
    "       'Lochhausen']].copy()\n",
    "    \n",
    "    # Add sine and cosine transformations of time-related features\n",
    "    time_features = ['hour', 'month', 'date_hour.dt.weekday']\n",
    "    for feature in time_features:\n",
    "        X[f'{feature}_sin'] = np.sin(2 * np.pi * X[feature] / X[feature].max())\n",
    "        X[f'{feature}_cos'] = np.cos(2 * np.pi * X[feature] / X[feature].max())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def scale_numeric_features(data: pd.DataFrame, numeric_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scales the numeric features in the given DataFrame using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "        numeric_features (list): List of column names representing the numeric features to scale.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with scaled numeric features.\n",
    "    \"\"\"\n",
    "    scaled_data = data.copy()\n",
    "\n",
    "    # Scale the numeric features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data[numeric_features] = scaler.fit_transform(scaled_data[numeric_features])\n",
    "\n",
    "    return scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (2802995, 3)\n",
      "(2802995, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STARTTIME</th>\n",
       "      <th>STARTLAT</th>\n",
       "      <th>STARTLON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 02:47:00</td>\n",
       "      <td>48.088402</td>\n",
       "      <td>11.48060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 04:00:00</td>\n",
       "      <td>48.105709</td>\n",
       "      <td>11.41446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 15:29:00</td>\n",
       "      <td>48.155258</td>\n",
       "      <td>11.54012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             STARTTIME   STARTLAT  STARTLON\n",
       "0  2019-01-01 02:47:00  48.088402  11.48060\n",
       "1  2019-01-01 04:00:00  48.105709  11.41446\n",
       "2  2019-01-01 15:29:00  48.155258  11.54012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the SQL query to fetch rental data from BigQuery\n",
    "query = f'''SELECT STARTTIME, STARTLAT, STARTLON\n",
    "         FROM `{GCP_PROJECT}.{BQ_DATASET}.raw_data_mvg`\n",
    "         WHERE STARTTIME >= '{START_YEAR}-01-01' AND STARTTIME <= '{END_YEAR}-12-31' \n",
    "         '''\n",
    "\n",
    "# Fetch the rental data from BigQuery\n",
    "cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"raw_{START_YEAR}_{END_YEAR}.csv\")\n",
    "rentals_df = get_raw_data(GCP_PROJECT, query, cache_path)\n",
    "print(rentals_df.shape)\n",
    "rentals_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2802995, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STARTTIME</th>\n",
       "      <th>STARTLAT</th>\n",
       "      <th>STARTLON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 02:47:00</td>\n",
       "      <td>48.088402</td>\n",
       "      <td>11.48060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 04:00:00</td>\n",
       "      <td>48.105709</td>\n",
       "      <td>11.41446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 15:29:00</td>\n",
       "      <td>48.155258</td>\n",
       "      <td>11.54012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            STARTTIME   STARTLAT  STARTLON\n",
       "0 2019-01-01 02:47:00  48.088402  11.48060\n",
       "1 2019-01-01 04:00:00  48.105709  11.41446\n",
       "2 2019-01-01 15:29:00  48.155258  11.54012"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the rental data\n",
    "rental_df_processed = pre_process_rental_df(rentals_df)\n",
    "print(rental_df_processed.shape)\n",
    "# rental_df_processed.to_csv('../raw_data/preprocesses_rental_df.csv')\n",
    "rental_df_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicate Rows: 23391\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Check\n",
    "duplicate_count = rental_df_processed.duplicated().sum()\n",
    "print(\"Number of Duplicate Rows:\", duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Load polygons from the file\n",
    "polygons_file_path = '../raw_data/polygons.csv'\n",
    "polygons = load_polygons(polygons_file_path)\n",
    "print(len(polygons))\n",
    "\n",
    "# Perform spatial join between rental data and polygons\n",
    "#rental_geo_df = get_district(rental_df_processed, polygons)\n",
    "#print(rental_geo_df.shape)\n",
    "# rental_geo_df.to_csv('../raw_data/rental_geo_df.csv')\n",
    "#rental_geo_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rental_geo_df = pd.read_csv('../raw_data/rental_geo_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Encode district labels using one-hot encoding\n",
    "encoded_rental_df = encode_district_label(rental_df_processed, polygons)\n",
    "print(encoded_rental_df.shape)\n",
    "encoded_rental_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rental data by hour\n",
    "rental_df_by_hour = group_rental_data_by_hour(encoded_rental_df)\n",
    "\n",
    "# Retrieve weather information and merge with rental data\n",
    "rental_df_all = get_weather_info(rental_df_by_hour)\n",
    "\n",
    "# Perform feature engineering on the merged data\n",
    "X, y = feature_extraction(rental_df_all)\n",
    "\n",
    "# Scale the numeric features in X using StandardScaler\n",
    "X_scaled = scale_numeric_features(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike_sharing_demand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
